{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@IwriteDSblog/gradient-descent-for-logistics-regression-in-python-18e033775082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXvector(X):\n",
    "    \"\"\" Taking the original independent variables matrix and add a row of 1 which corresponds to x_0\n",
    "        Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: the matrix that contains all the values in the dataset, not include the outcomes variables. \n",
    "    \"\"\"    \n",
    "    vectorX = np.c_[np.ones((len(X), 1)), X]\n",
    "    return vectorX\n",
    "\n",
    "def theta_init(X):\n",
    "    \"\"\" Generate an initial value of vector Î¸ from the original independent variables matrix\n",
    "         Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: a vector of theta filled with initial guess\n",
    "    \"\"\"\n",
    "    theta = np.random.randn(len(X[0])+1, 1)\n",
    "    return theta\n",
    "def sigmoid_function(X):\n",
    "    \"\"\" Calculate the sigmoid value of the inputs\n",
    "         Parameters:\n",
    "          X:  values\n",
    "        Return value: the sigmoid value\n",
    "    \"\"\"\n",
    "    return 1/(1+math.e**(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistics_Regression(X,y,learningrate, iterations):\n",
    "    \"\"\" Find the Logistics regression model for the data set\n",
    "         Parameters:\n",
    "          X: independent variables matrix\n",
    "          y: dependent variables matrix\n",
    "          learningrate: learningrate of Gradient Descent\n",
    "          iterations: the number of iterations\n",
    "        Return value: the final theta vector and the plot of cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    y_new = np.reshape(y, (len(y), 1))   \n",
    "    cost_lst = []\n",
    "    vectorX = generateXvector(X)\n",
    "    theta = theta_init(X)\n",
    "    m = len(X)\n",
    "    print(f\"vectorX {vectorX} y {y}\")\n",
    "    print(f\"vectorX.shape {vectorX.shape} y_new.shape {y_new.shape}\")\n",
    "    for i in range(iterations):\n",
    "        gradients = 2/m * vectorX.T.dot(sigmoid_function(vectorX.dot(theta)) - y_new)\n",
    "        print(f\"grad.shape {gradients.shape} w.shape {theta.shape}\")\n",
    "        theta = theta - learningrate * gradients\n",
    "        y_pred = sigmoid_function(vectorX.dot(theta))\n",
    "        cost_value = - np.sum(np.dot(y_new.T,np.log(y_pred)+ np.dot((1-y_new).T,np.log(1-y_pred)))) /(len(y_pred))\n",
    " #Calculate the loss for each training instance\n",
    "        cost_lst.append(cost_value)\n",
    "    plt.plot(np.arange(1,iterations),cost_lst[1:], color = 'red')\n",
    "    plt.title('Cost function Graph')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b6ac87d17fff>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris[\"target\"] == 0).astype(np.int) #return 1 if Iris Versicolor, else 0.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = (iris[\"target\"] == 0).astype(np.int) #return 1 if Iris Versicolor, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-11.07402312]),\n",
       " array([[ -1.32289075,   4.23503694, -10.11887281,  -9.22137322]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0, penalty = 'none')\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.intercept_, classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    \"\"\" Returning all the values in a specific columns\n",
    "         Parameters:\n",
    "          X: the input matrix\n",
    "          i: the column\n",
    "     Return value: an array with desired column\n",
    "    \"\"\"\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def accuracy_LR(X,y,learningrate, iteration,X_test, y_test):\n",
    "    \"\"\" Returning the accuracy score for a training model\n",
    "    \n",
    "    \"\"\"\n",
    "    ideal = Logistics_Regression(X,y,learningrate, iteration)\n",
    "    hypo_line = ideal[0]\n",
    "    for i in range(1,len(ideal)):\n",
    "        hypo_line = hypo_line + ideal[i]*column(X_test,i-1)\n",
    "    logistic_function = sigmoid_function(hypo_line)\n",
    "    for i in range(len(logistic_function)):\n",
    "        if logistic_function[i] >= 0.5:\n",
    "            logistic_function[i] = 1\n",
    "        else:\n",
    "            logistic_function[i] = 0\n",
    "    last1 = np.concatenate((logistic_function.reshape(len(logistic_function),1), y_test.reshape(len(y_test),1)),1)\n",
    "    count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if last1[i][0] == last1[i][1]:\n",
    "            count = count+1\n",
    "    acc = count/(len(y_test))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorX [[ 1.          0.61303014  0.10850105  0.94751783  0.736072  ]\n",
      " [ 1.         -0.56776627 -0.12400121  0.38491447  0.34752959]\n",
      " [ 1.         -0.80392556  1.03851009 -1.30289562 -1.33615415]\n",
      " [ 1.          0.25879121 -0.12400121  0.60995581  0.736072  ]\n",
      " [ 1.          0.61303014 -0.58900572  1.00377816  1.25412853]\n",
      " [ 1.         -0.80392556 -0.82150798  0.04735245  0.21801546]\n",
      " [ 1.         -0.21352735  1.73601687 -1.19037495 -1.20664002]\n",
      " [ 1.          0.14071157 -0.82150798  0.72247648  0.47704373]\n",
      " [ 1.          0.02263193 -0.12400121  0.21613346  0.34752959]\n",
      " [ 1.         -0.09544771 -1.05401024  0.10361279 -0.04101281]\n",
      " [ 1.          1.0853487  -0.12400121  0.94751783  1.1246144 ]\n",
      " [ 1.         -1.39432376  0.34100331 -1.41541629 -1.33615415]\n",
      " [ 1.          1.20342834  0.10850105  0.72247648  1.38364267]\n",
      " [ 1.         -1.04008484  1.03851009 -1.24663528 -0.81809761]\n",
      " [ 1.         -0.56776627  1.50351461 -1.30289562 -1.33615415]\n",
      " [ 1.         -1.04008484 -2.4490238  -0.1776889  -0.30004108]\n",
      " [ 1.          0.73110978 -0.12400121  0.94751783  0.736072  ]\n",
      " [ 1.          0.96726906  0.57350557  1.0600385   1.64267094]\n",
      " [ 1.          0.14071157 -1.98401928  0.66621615  0.34752959]\n",
      " [ 1.          0.96726906 -1.2865125   1.11629884  0.736072  ]\n",
      " [ 1.         -0.33160699 -1.2865125   0.04735245 -0.17052694]\n",
      " [ 1.          2.14806547 -0.12400121  1.28507985  1.38364267]\n",
      " [ 1.          0.49495049  0.57350557  0.49743514  0.47704373]\n",
      " [ 1.         -0.44968663 -1.51901476 -0.00890789 -0.17052694]\n",
      " [ 1.          0.49495049 -0.82150798  0.60995581  0.736072  ]\n",
      " [ 1.          0.49495049 -0.58900572  0.72247648  0.34752959]\n",
      " [ 1.         -1.15816448 -1.2865125   0.38491447  0.60655786]\n",
      " [ 1.          0.49495049 -1.2865125   0.66621615  0.86558613]\n",
      " [ 1.          1.32150798  0.34100331  0.49743514  0.21801546]\n",
      " [ 1.          0.73110978 -0.12400121  0.77873682  0.99510027]\n",
      " [ 1.          0.14071157  0.80600783  0.38491447  0.47704373]\n",
      " [ 1.         -1.27624412  0.10850105 -1.24663528 -1.33615415]\n",
      " [ 1.         -0.09544771 -0.82150798  0.72247648  0.86558613]\n",
      " [ 1.         -0.33160699 -0.82150798  0.21613346  0.08850133]\n",
      " [ 1.         -0.33160699 -0.35650346 -0.12142856  0.08850133]\n",
      " [ 1.         -0.44968663 -1.2865125   0.10361279  0.08850133]\n",
      " [ 1.          0.25879121 -0.12400121  0.4411748   0.21801546]\n",
      " [ 1.          1.55766726  0.34100331  1.22881951  0.736072  ]\n",
      " [ 1.         -0.68584591  1.50351461 -1.30289562 -1.33615415]\n",
      " [ 1.         -1.86664232 -0.12400121 -1.52793696 -1.46566829]\n",
      " [ 1.          0.61303014 -0.82150798  0.83499716  0.86558613]\n",
      " [ 1.         -0.21352735 -0.12400121  0.21613346 -0.04101281]\n",
      " [ 1.         -0.56776627  0.80600783 -1.19037495 -1.33615415]\n",
      " [ 1.         -0.21352735  3.13103043 -1.30289562 -1.07712588]\n",
      " [ 1.          1.20342834  0.10850105  0.60995581  0.34752959]\n",
      " [ 1.         -1.5124034   0.10850105 -1.30289562 -1.33615415]\n",
      " [ 1.          0.02263193 -0.12400121  0.72247648  0.736072  ]\n",
      " [ 1.         -0.9220052  -1.2865125  -0.45899058 -0.17052694]\n",
      " [ 1.         -1.5124034   0.80600783 -1.35915595 -1.20664002]\n",
      " [ 1.          0.37687085 -1.98401928  0.38491447  0.34752959]\n",
      " [ 1.          1.55766726  1.27101235  1.28507985  1.64267094]\n",
      " [ 1.         -0.21352735 -0.35650346  0.21613346  0.08850133]\n",
      " [ 1.         -1.27624412 -0.12400121 -1.35915595 -1.46566829]\n",
      " [ 1.          1.43958762 -0.12400121  1.17255917  1.1246144 ]\n",
      " [ 1.          1.20342834  0.34100331  1.0600385   1.38364267]\n",
      " [ 1.          0.73110978 -0.12400121  1.11629884  1.25412853]\n",
      " [ 1.          0.61303014 -0.58900572  1.00377816  1.1246144 ]\n",
      " [ 1.         -0.9220052   1.73601687 -1.24663528 -1.33615415]\n",
      " [ 1.         -1.27624412  0.80600783 -1.24663528 -1.33615415]\n",
      " [ 1.          0.73110978  0.34100331  0.72247648  0.99510027]\n",
      " [ 1.          0.96726906  0.57350557  1.0600385   1.1246144 ]\n",
      " [ 1.         -1.63048304 -1.75151702 -1.41541629 -1.20664002]\n",
      " [ 1.          0.37687085  0.80600783  0.89125749  1.38364267]\n",
      " [ 1.         -1.15816448 -0.12400121 -1.35915595 -1.33615415]\n",
      " [ 1.         -0.21352735 -1.2865125   0.66621615  0.99510027]\n",
      " [ 1.          1.20342834  0.10850105  0.89125749  1.1246144 ]\n",
      " [ 1.         -1.74856268  0.34100331 -1.41541629 -1.33615415]\n",
      " [ 1.         -1.04008484  1.27101235 -1.35915595 -1.33615415]\n",
      " [ 1.          1.55766726 -0.12400121  1.11629884  0.47704373]\n",
      " [ 1.         -0.9220052   1.03851009 -1.35915595 -1.20664002]\n",
      " [ 1.         -1.74856268 -0.12400121 -1.41541629 -1.33615415]\n",
      " [ 1.         -0.56776627  1.96851913 -1.19037495 -1.07712588]\n",
      " [ 1.         -0.44968663 -1.75151702  0.10361279  0.08850133]\n",
      " [ 1.          1.0853487   0.34100331  1.17255917  1.38364267]\n",
      " [ 1.          2.02998583 -0.12400121  1.56638153  1.1246144 ]\n",
      " [ 1.         -0.9220052   1.03851009 -1.35915595 -1.33615415]\n",
      " [ 1.         -1.15816448  0.10850105 -1.30289562 -1.33615415]\n",
      " [ 1.         -0.80392556  0.80600783 -1.35915595 -1.33615415]\n",
      " [ 1.         -0.21352735 -0.58900572  0.38491447  0.08850133]\n",
      " [ 1.          0.84918942 -0.12400121  0.32865413  0.21801546]\n",
      " [ 1.         -1.04008484  0.34100331 -1.47167663 -1.33615415]\n",
      " [ 1.         -0.9220052   0.57350557 -1.19037495 -0.94761175]\n",
      " [ 1.          0.61303014 -0.35650346  0.27239379  0.08850133]\n",
      " [ 1.         -0.56776627  0.80600783 -1.30289562 -1.07712588]\n",
      " [ 1.          2.14806547 -1.05401024  1.73516253  1.38364267]\n",
      " [ 1.         -1.15816448 -1.51901476 -0.29020957 -0.30004108]\n",
      " [ 1.          2.38422475  1.73601687  1.45386085  0.99510027]\n",
      " [ 1.          0.96726906  0.10850105  0.32865413  0.21801546]\n",
      " [ 1.         -0.80392556  2.43352365 -1.30289562 -1.46566829]\n",
      " [ 1.          0.14071157 -0.12400121  0.55369548  0.736072  ]\n",
      " [ 1.         -0.09544771  2.20102139 -1.47167663 -1.33615415]\n",
      " [ 1.          2.14806547 -0.58900572  1.62264186  0.99510027]\n",
      " [ 1.         -0.9220052   1.73601687 -1.30289562 -1.20664002]\n",
      " [ 1.         -1.39432376  0.34100331 -1.24663528 -1.33615415]\n",
      " [ 1.          1.79382654 -0.58900572  1.28507985  0.86558613]\n",
      " [ 1.         -1.04008484  0.57350557 -1.35915595 -1.33615415]\n",
      " [ 1.          0.49495049  0.80600783  1.00377816  1.5131568 ]\n",
      " [ 1.         -0.21352735 -0.58900572  0.15987312  0.08850133]\n",
      " [ 1.         -0.09544771 -0.82150798  0.04735245 -0.04101281]\n",
      " [ 1.         -0.21352735 -1.05401024 -0.1776889  -0.30004108]\n",
      " [ 1.          0.61303014  0.34100331  0.83499716  1.38364267]\n",
      " [ 1.          0.96726906 -0.12400121  0.77873682  1.38364267]\n",
      " [ 1.          0.49495049 -1.2865125   0.60995581  0.34752959]\n",
      " [ 1.          0.96726906 -0.12400121  0.66621615  0.60655786]\n",
      " [ 1.         -1.04008484 -0.12400121 -1.24663528 -1.33615415]\n",
      " [ 1.         -0.44968663 -1.51901476 -0.06516822 -0.30004108]\n",
      " [ 1.          0.96726906  0.10850105  1.00377816  1.5131568 ]\n",
      " [ 1.         -0.09544771 -0.82150798  0.72247648  0.86558613]\n",
      " [ 1.         -0.9220052   0.80600783 -1.30289562 -1.33615415]\n",
      " [ 1.          0.84918942 -0.35650346  0.4411748   0.08850133]\n",
      " [ 1.         -0.33160699 -0.12400121  0.15987312  0.08850133]\n",
      " [ 1.          0.02263193  0.34100331  0.55369548  0.736072  ]\n",
      " [ 1.          0.49495049 -1.75151702  0.32865413  0.08850133]\n",
      " [ 1.         -0.44968663  1.03851009 -1.41541629 -1.33615415]\n",
      " [ 1.         -0.9220052   1.50351461 -1.30289562 -1.07712588]\n",
      " [ 1.         -1.15816448  0.10850105 -1.30289562 -1.46566829]\n",
      " [ 1.          0.49495049 -0.35650346  1.00377816  0.736072  ]\n",
      " [ 1.         -0.09544771 -0.82150798  0.15987312 -0.30004108]\n",
      " [ 1.          2.14806547  1.73601687  1.62264186  1.25412853]\n",
      " [ 1.         -1.5124034   0.34100331 -1.35915595 -1.33615415]] y [0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
      " 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 1 1 0 0 0 1]\n",
      "vectorX.shape (120, 5) y_new.shape (120, 1)\n",
      "grad.shape (5, 1) w.shape (5, 1)\n",
      "grad.shape (5, 1) w.shape (5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.33194485],\n",
       "       [-0.18047227],\n",
       "       [ 0.36660262],\n",
       "       [-1.50001835],\n",
       "       [-0.95738733]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistics_Regression(X_train,y_train, 1, 2) #1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorX [[ 1.          0.61303014  0.10850105  0.94751783  0.736072  ]\n",
      " [ 1.         -0.56776627 -0.12400121  0.38491447  0.34752959]\n",
      " [ 1.         -0.80392556  1.03851009 -1.30289562 -1.33615415]\n",
      " [ 1.          0.25879121 -0.12400121  0.60995581  0.736072  ]\n",
      " [ 1.          0.61303014 -0.58900572  1.00377816  1.25412853]\n",
      " [ 1.         -0.80392556 -0.82150798  0.04735245  0.21801546]\n",
      " [ 1.         -0.21352735  1.73601687 -1.19037495 -1.20664002]\n",
      " [ 1.          0.14071157 -0.82150798  0.72247648  0.47704373]\n",
      " [ 1.          0.02263193 -0.12400121  0.21613346  0.34752959]\n",
      " [ 1.         -0.09544771 -1.05401024  0.10361279 -0.04101281]\n",
      " [ 1.          1.0853487  -0.12400121  0.94751783  1.1246144 ]\n",
      " [ 1.         -1.39432376  0.34100331 -1.41541629 -1.33615415]\n",
      " [ 1.          1.20342834  0.10850105  0.72247648  1.38364267]\n",
      " [ 1.         -1.04008484  1.03851009 -1.24663528 -0.81809761]\n",
      " [ 1.         -0.56776627  1.50351461 -1.30289562 -1.33615415]\n",
      " [ 1.         -1.04008484 -2.4490238  -0.1776889  -0.30004108]\n",
      " [ 1.          0.73110978 -0.12400121  0.94751783  0.736072  ]\n",
      " [ 1.          0.96726906  0.57350557  1.0600385   1.64267094]\n",
      " [ 1.          0.14071157 -1.98401928  0.66621615  0.34752959]\n",
      " [ 1.          0.96726906 -1.2865125   1.11629884  0.736072  ]\n",
      " [ 1.         -0.33160699 -1.2865125   0.04735245 -0.17052694]\n",
      " [ 1.          2.14806547 -0.12400121  1.28507985  1.38364267]\n",
      " [ 1.          0.49495049  0.57350557  0.49743514  0.47704373]\n",
      " [ 1.         -0.44968663 -1.51901476 -0.00890789 -0.17052694]\n",
      " [ 1.          0.49495049 -0.82150798  0.60995581  0.736072  ]\n",
      " [ 1.          0.49495049 -0.58900572  0.72247648  0.34752959]\n",
      " [ 1.         -1.15816448 -1.2865125   0.38491447  0.60655786]\n",
      " [ 1.          0.49495049 -1.2865125   0.66621615  0.86558613]\n",
      " [ 1.          1.32150798  0.34100331  0.49743514  0.21801546]\n",
      " [ 1.          0.73110978 -0.12400121  0.77873682  0.99510027]\n",
      " [ 1.          0.14071157  0.80600783  0.38491447  0.47704373]\n",
      " [ 1.         -1.27624412  0.10850105 -1.24663528 -1.33615415]\n",
      " [ 1.         -0.09544771 -0.82150798  0.72247648  0.86558613]\n",
      " [ 1.         -0.33160699 -0.82150798  0.21613346  0.08850133]\n",
      " [ 1.         -0.33160699 -0.35650346 -0.12142856  0.08850133]\n",
      " [ 1.         -0.44968663 -1.2865125   0.10361279  0.08850133]\n",
      " [ 1.          0.25879121 -0.12400121  0.4411748   0.21801546]\n",
      " [ 1.          1.55766726  0.34100331  1.22881951  0.736072  ]\n",
      " [ 1.         -0.68584591  1.50351461 -1.30289562 -1.33615415]\n",
      " [ 1.         -1.86664232 -0.12400121 -1.52793696 -1.46566829]\n",
      " [ 1.          0.61303014 -0.82150798  0.83499716  0.86558613]\n",
      " [ 1.         -0.21352735 -0.12400121  0.21613346 -0.04101281]\n",
      " [ 1.         -0.56776627  0.80600783 -1.19037495 -1.33615415]\n",
      " [ 1.         -0.21352735  3.13103043 -1.30289562 -1.07712588]\n",
      " [ 1.          1.20342834  0.10850105  0.60995581  0.34752959]\n",
      " [ 1.         -1.5124034   0.10850105 -1.30289562 -1.33615415]\n",
      " [ 1.          0.02263193 -0.12400121  0.72247648  0.736072  ]\n",
      " [ 1.         -0.9220052  -1.2865125  -0.45899058 -0.17052694]\n",
      " [ 1.         -1.5124034   0.80600783 -1.35915595 -1.20664002]\n",
      " [ 1.          0.37687085 -1.98401928  0.38491447  0.34752959]\n",
      " [ 1.          1.55766726  1.27101235  1.28507985  1.64267094]\n",
      " [ 1.         -0.21352735 -0.35650346  0.21613346  0.08850133]\n",
      " [ 1.         -1.27624412 -0.12400121 -1.35915595 -1.46566829]\n",
      " [ 1.          1.43958762 -0.12400121  1.17255917  1.1246144 ]\n",
      " [ 1.          1.20342834  0.34100331  1.0600385   1.38364267]\n",
      " [ 1.          0.73110978 -0.12400121  1.11629884  1.25412853]\n",
      " [ 1.          0.61303014 -0.58900572  1.00377816  1.1246144 ]\n",
      " [ 1.         -0.9220052   1.73601687 -1.24663528 -1.33615415]\n",
      " [ 1.         -1.27624412  0.80600783 -1.24663528 -1.33615415]\n",
      " [ 1.          0.73110978  0.34100331  0.72247648  0.99510027]\n",
      " [ 1.          0.96726906  0.57350557  1.0600385   1.1246144 ]\n",
      " [ 1.         -1.63048304 -1.75151702 -1.41541629 -1.20664002]\n",
      " [ 1.          0.37687085  0.80600783  0.89125749  1.38364267]\n",
      " [ 1.         -1.15816448 -0.12400121 -1.35915595 -1.33615415]\n",
      " [ 1.         -0.21352735 -1.2865125   0.66621615  0.99510027]\n",
      " [ 1.          1.20342834  0.10850105  0.89125749  1.1246144 ]\n",
      " [ 1.         -1.74856268  0.34100331 -1.41541629 -1.33615415]\n",
      " [ 1.         -1.04008484  1.27101235 -1.35915595 -1.33615415]\n",
      " [ 1.          1.55766726 -0.12400121  1.11629884  0.47704373]\n",
      " [ 1.         -0.9220052   1.03851009 -1.35915595 -1.20664002]\n",
      " [ 1.         -1.74856268 -0.12400121 -1.41541629 -1.33615415]\n",
      " [ 1.         -0.56776627  1.96851913 -1.19037495 -1.07712588]\n",
      " [ 1.         -0.44968663 -1.75151702  0.10361279  0.08850133]\n",
      " [ 1.          1.0853487   0.34100331  1.17255917  1.38364267]\n",
      " [ 1.          2.02998583 -0.12400121  1.56638153  1.1246144 ]\n",
      " [ 1.         -0.9220052   1.03851009 -1.35915595 -1.33615415]\n",
      " [ 1.         -1.15816448  0.10850105 -1.30289562 -1.33615415]\n",
      " [ 1.         -0.80392556  0.80600783 -1.35915595 -1.33615415]\n",
      " [ 1.         -0.21352735 -0.58900572  0.38491447  0.08850133]\n",
      " [ 1.          0.84918942 -0.12400121  0.32865413  0.21801546]\n",
      " [ 1.         -1.04008484  0.34100331 -1.47167663 -1.33615415]\n",
      " [ 1.         -0.9220052   0.57350557 -1.19037495 -0.94761175]\n",
      " [ 1.          0.61303014 -0.35650346  0.27239379  0.08850133]\n",
      " [ 1.         -0.56776627  0.80600783 -1.30289562 -1.07712588]\n",
      " [ 1.          2.14806547 -1.05401024  1.73516253  1.38364267]\n",
      " [ 1.         -1.15816448 -1.51901476 -0.29020957 -0.30004108]\n",
      " [ 1.          2.38422475  1.73601687  1.45386085  0.99510027]\n",
      " [ 1.          0.96726906  0.10850105  0.32865413  0.21801546]\n",
      " [ 1.         -0.80392556  2.43352365 -1.30289562 -1.46566829]\n",
      " [ 1.          0.14071157 -0.12400121  0.55369548  0.736072  ]\n",
      " [ 1.         -0.09544771  2.20102139 -1.47167663 -1.33615415]\n",
      " [ 1.          2.14806547 -0.58900572  1.62264186  0.99510027]\n",
      " [ 1.         -0.9220052   1.73601687 -1.30289562 -1.20664002]\n",
      " [ 1.         -1.39432376  0.34100331 -1.24663528 -1.33615415]\n",
      " [ 1.          1.79382654 -0.58900572  1.28507985  0.86558613]\n",
      " [ 1.         -1.04008484  0.57350557 -1.35915595 -1.33615415]\n",
      " [ 1.          0.49495049  0.80600783  1.00377816  1.5131568 ]\n",
      " [ 1.         -0.21352735 -0.58900572  0.15987312  0.08850133]\n",
      " [ 1.         -0.09544771 -0.82150798  0.04735245 -0.04101281]\n",
      " [ 1.         -0.21352735 -1.05401024 -0.1776889  -0.30004108]\n",
      " [ 1.          0.61303014  0.34100331  0.83499716  1.38364267]\n",
      " [ 1.          0.96726906 -0.12400121  0.77873682  1.38364267]\n",
      " [ 1.          0.49495049 -1.2865125   0.60995581  0.34752959]\n",
      " [ 1.          0.96726906 -0.12400121  0.66621615  0.60655786]\n",
      " [ 1.         -1.04008484 -0.12400121 -1.24663528 -1.33615415]\n",
      " [ 1.         -0.44968663 -1.51901476 -0.06516822 -0.30004108]\n",
      " [ 1.          0.96726906  0.10850105  1.00377816  1.5131568 ]\n",
      " [ 1.         -0.09544771 -0.82150798  0.72247648  0.86558613]\n",
      " [ 1.         -0.9220052   0.80600783 -1.30289562 -1.33615415]\n",
      " [ 1.          0.84918942 -0.35650346  0.4411748   0.08850133]\n",
      " [ 1.         -0.33160699 -0.12400121  0.15987312  0.08850133]\n",
      " [ 1.          0.02263193  0.34100331  0.55369548  0.736072  ]\n",
      " [ 1.          0.49495049 -1.75151702  0.32865413  0.08850133]\n",
      " [ 1.         -0.44968663  1.03851009 -1.41541629 -1.33615415]\n",
      " [ 1.         -0.9220052   1.50351461 -1.30289562 -1.07712588]\n",
      " [ 1.         -1.15816448  0.10850105 -1.30289562 -1.46566829]\n",
      " [ 1.          0.49495049 -0.35650346  1.00377816  0.736072  ]\n",
      " [ 1.         -0.09544771 -0.82150798  0.15987312 -0.30004108]\n",
      " [ 1.          2.14806547  1.73601687  1.62264186  1.25412853]\n",
      " [ 1.         -1.5124034   0.34100331 -1.35915595 -1.33615415]] y [0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
      " 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 1 1 0 0 0 1]\n",
      "vectorX.shape (120, 5) y_new.shape (120, 1)\n",
      "grad.shape (5, 1) w.shape (5, 1)\n",
      "grad.shape (5, 1) w.shape (5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LR(X_train,y_train, 1, 2,X_test, y_test) #1000000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
