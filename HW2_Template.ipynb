{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: There will be no extension of the homework deadline. E-mails sent for extension requests will not be considered. So please start working early on your homework and manage your time well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Optimization (75 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "**Gradient Descent**: the parameters are updated in the direction of negative gradient. In equation form: \n",
    "### <center> $ \\textbf{w}^k =\\textbf{w}^{k-1} - \\alpha \\nabla g(\\textbf{w}^{k-1}) $ <center> \n",
    "\n",
    "    \n",
    "**Newton's method** finds the local minima by moving towards to stationary points of second order Taylor series approximation, with the following update formula:\n",
    "### <center> $\\textbf{w}^k = \\textbf{w}^{k-1} - (\\nabla^2 g(\\textbf{w}^{k-1}))^{-1} \\nabla g(\\textbf{w}^{k-1})$ <center>\n",
    "    \n",
    "In this part, you will try to optimize given 2 multivariate functions using Gradient Descent and Newton's Method.\n",
    "   \n",
    "- $g(\\textbf{w}) = -sin(\\pi \\textbf{w}^T \\textbf{w}) + ln(\\textbf{w}^T \\textbf{w}) $\n",
    "- $g(\\textbf{w}) = \\textbf{w}^T Q \\textbf{w} + r^T \\textbf{w}  $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Optimize Function 1 (30 Points)\n",
    "    \n",
    "We will experiment with both optimization methods on function 1:\n",
    "\n",
    "### <center> $g(\\textbf{w}) = -sin(\\pi \\textbf{w}^T \\textbf{w}) + ln(\\textbf{w}^T \\textbf{w}) $ </center>\n",
    "\n",
    "where w is a vector with 2 elements as $ \\textbf{w} = [\\textbf{w}_1    \\textbf{w}_2]^T $\n",
    "\n",
    "You are given the following plotter function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colorspec(w_hist):\n",
    "    # make color range for path\n",
    "    s = np.linspace(0,1,len(w_hist[:round(len(w_hist)/2)]))\n",
    "    s.shape = (len(s),1)\n",
    "    t = np.ones(len(w_hist[round(len(w_hist)/2):]))\n",
    "    t.shape = (len(t),1)\n",
    "    s = np.vstack((s,t))\n",
    "    colorspec = []\n",
    "    colorspec = np.concatenate((s,np.flipud(s)),1)\n",
    "    colorspec = np.concatenate((colorspec,np.zeros((len(s),1))),1)\n",
    "    return colorspec\n",
    "\n",
    "def draw_weight_path(ax,w_hist,g):\n",
    "    # make colors for plot\n",
    "    colorspec = make_colorspec(w_hist)\n",
    "    \n",
    "    arrows = True\n",
    "\n",
    "    ### plot function decrease plot in right panel\n",
    "    for j in range(len(w_hist)):  \n",
    "        w_val = w_hist[j]\n",
    "\n",
    "        # plot each weight set as a point\n",
    "        ax.scatter(w_val[0],w_val[1],g(w_val),s = 80,color = colorspec[j],edgecolor = 'black',linewidth = 2*math.sqrt((1/(float(j) + 1))),zorder = 3)\n",
    "        \n",
    "        # plot connector between points for visualization purposes\n",
    "        if j > 0:\n",
    "            pt1 = np.array([w_hist[j-1][0], w_hist[j-1][1], np.squeeze(g(w_hist[j-1]))])\n",
    "            pt2 = np.array([w_hist[j][0], w_hist[j][1], np.squeeze(g(w_hist[j]))])\n",
    "            \n",
    "            # if points are different draw error\n",
    "            if np.linalg.norm(pt1 - pt2) > 0.1 and arrows == True:\n",
    "                \n",
    "                ax.plot([pt1[0],pt2[0]], [pt1[1], pt2[1]],[pt1[2],pt2[2]] ,  color ='black',linewidth = 2, zorder=2)\n",
    "                \n",
    "\n",
    "def plotFunction(g, w_history=None, view=(50,80),w_min=-1.0,w_max=1.0):\n",
    "    x = np.linspace(w_min, w_max, 100)\n",
    "    y = np.linspace(w_min, w_max, 100)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    Z = np.empty([100,100])\n",
    "    vectorize_w = lambda w0, w1: np.array([[w0], [w1]])\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            Z[i,j] = np.squeeze(g(vectorize_w(X[i,j], Y[i,j])))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.view_init(view[0], view[1])\n",
    "    \n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('w1')\n",
    "    ax.set_ylabel('w2')\n",
    "    ax.set_zlabel('g(w)')\n",
    "    ax.set_title('Function surface');\n",
    "    \n",
    "    if w_history:\n",
    "        draw_weight_path(ax, w_history, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define and plot the given function in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the function 1\n",
    "g = lambda w: -np.sin(np.pi*np.sum(w**2)) + np.log(np.sum(w**2))\n",
    "\n",
    "# Now, we can call the plotFunction to plot surface plot\n",
    "plotFunction(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points)** Complete the following **gradient descent** function.\n",
    " - You need to write down the first derivative function for the function g above and implement the update rule. You need to calculate the gradient by yourself!\n",
    " - You need to take steps until the stopping criteria is met. **Stopping criteria** is the Euclidian distance between consecutive w vectors to get below 1e-02.\n",
    "    - Write the gradient descent loop as it breaks when the Euclidian distance between $w_{k-1}$ and $w_{k}$ is smaller than 1e-02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent_func1(g, lr = 1e-1, w_0 = None):\n",
    "    '''\n",
    "        lr: learning rate\n",
    "        w_0: initial w\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    w = w_0\n",
    "    w_history = [w] #List of w's in each iteration, used to plot\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    # Code the gradient(w) function here.\n",
    "    gradient = lambda w: ...\n",
    "    \n",
    "    #Gradient Descent Steps until the stopping criteria is met\n",
    "    \n",
    "    ...\n",
    "         \n",
    "    ### YOUR CODE ENDS HERE    \n",
    "        \n",
    "    return w, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run and test the gradient descent function with given w_0 and learning rate parameters and plot the weight path at the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, w_history = gradientDescent_func1(g, lr=0.02, w_0=np.array([0.5,0.5]))\n",
    "\n",
    "plotFunction(g, w_history, view=(65,75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Complete the gradient descent function, but this time, use **adaptive learning rate(step size)** instead of a constant learning rate (Refer to the tutorial 4 for adaptive learning rate strategies).  \n",
    "\n",
    "Update the learning rate using the following formula:\n",
    "\n",
    "$  \\alpha = \\dfrac{\\alpha}{(1 + t * d)}$ \n",
    "\n",
    "$  t = t + 1 $\n",
    "\n",
    "where $\\alpha$ is the learning rate, $t$ is the time step, starting from 0 and increasing by 1 at each iteration and $d$ is a hyperparameter controlling how quickly the learning rate decays.\n",
    "\n",
    " - You can copy part of codes from the first gradient descent function above as the gradient and the update rule stays the same\n",
    " - As above, You need to take steps until the stopping criteria is met. **Stopping criteria** is the Euclidian distance between consecutive w vectors to get below 1e-02.\n",
    "    - Write the gradient descent loop as it breaks when the Euclidian distance between $w_{k-1}$ and $w_{k}$ is smaller than 1e-02.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent_func1_adaptiveLR(g, lr = 1e-1, w_0 = None, d_init = 0.1):\n",
    "    '''\n",
    "        lr: learning rate\n",
    "        w_0: initial w\n",
    "        d_init: initial d\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    w = w_0\n",
    "    w_history = [w] #List of w's in each iteration, used to plot\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    # Code the gradient(w) function here.\n",
    "    gradient = lambda w: ...\n",
    "    \n",
    "    #Gradient Descent Steps until the stopping criteria is met (Don't forget to update Learning Rate!)\n",
    "    ...\n",
    "    #\n",
    "        \n",
    "    ### YOUR CODE ENDS HERE    \n",
    "        \n",
    "    return w, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run and test the gradient descent function with given w_0 and learning rate parameters and plot the weight path at the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, w_history = gradientDescent_func1_adaptiveLR(g, lr=0.03, w_0=np.array([0.5,0.5]), d_init=0.15)\n",
    "\n",
    "plotFunction(g, w_history, view=(65,75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Answer following questions.\n",
    " - Comparing both of the approachs, what did you notice? What were the main differences between both approaches? Explain in detail. Which approach worked better? Why?\n",
    " - Could it be better to use Newton's method to optimize this function? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Double click to insert your answer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Optimize Function 2 (45 Points)\n",
    "    \n",
    "We will experiment with both optimization methods on function 2:\n",
    "\n",
    "### <center> $g(\\textbf{w}) = \\textbf{w}^T Q \\textbf{w} + r^T \\textbf{w} $ </center>\n",
    "\n",
    "where w is a vector with 2 elements as $\\textbf{w} = \\begin{bmatrix} \\textbf{w}_1 \\\\ \\textbf{w}_2\\end{bmatrix} $, Q is a $2 \\times 2$ matrix as $Q =\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}$ and $ r =\\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "First, let's define and plot the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the function 2\n",
    "g = lambda w: (w.T @ np.array([[2,1], [1,2]]))@w + np.array([[2,5]])@w\n",
    "\n",
    "# Now, we can call the plotFunction to plot surface plot\n",
    "plotFunction(g,view=(40,-40),w_min=-10, w_max=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Calculate the Hessian matrix for the given function where the Hessian matrix will be in form of:\n",
    "\n",
    "### <center> $ H = \\begin{bmatrix} \\frac{\\partial^2 g}{\\partial^2 \\textbf{w}_1} & \\frac{\\partial^2 g}{\\partial \\textbf{w}_1 \\partial \\textbf{w}_2} \\\\ \\frac{\\partial^2 g}{\\partial \\textbf{w}_2 \\partial \\textbf{w}_1} & \\frac{\\partial^2 g}{\\partial^2 \\textbf{w}_2} \\end{bmatrix} $ </center>\n",
    "\n",
    "Hint: You can write the given quadratic function as: \n",
    "\n",
    "### <center> $g(\\textbf{w}) = \\begin{bmatrix}\\textbf{w}_1 & \\textbf{w}_2\\end{bmatrix} \\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix} \\begin{bmatrix}\\textbf{w}_1 \\\\ \\textbf{w}_2\\end{bmatrix} + \\begin{bmatrix}2 & 5\\end{bmatrix} \\begin{bmatrix}\\textbf{w}_1 \\\\ \\textbf{w}_2\\end{bmatrix} $ </center>\n",
    "\n",
    "Calculate the given derivatives, fill the Hessian matrix and find the eigenvalues of the Hessian matrix with step-by-step solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial g}{\\partial \\textbf{w}_1} = ? $\n",
    "$ \\frac{\\partial g}{\\partial \\textbf{w}_2} = ? $\n",
    "$ \\frac{\\partial^2 g}{\\partial^2 \\textbf{w}_1} = ? $\n",
    "$ \\frac{\\partial^2 g}{\\partial \\textbf{w}_1 \\partial \\textbf{w}_2} = ? $\n",
    "$\\frac{\\partial^2 g}{\\partial \\textbf{w}_2 \\partial \\textbf{w}_1} = ? $\n",
    "$\\frac{\\partial^2 g}{\\partial^2 \\textbf{w}_2} = ? $ \n",
    "\n",
    "\n",
    "\n",
    "### <center> $ H = \\begin{bmatrix} ? & ? \\\\ ? & ? \\end{bmatrix} $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Calculate eigenvalues here. Write down your solution step-by-step._\n",
    "\n",
    "### <center> $ \\lambda_1 = ? $ $\\lambda_2 = ? $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is is safe to use Newton's method for optimizing this function? Comment on it referring to the calculations you made before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Double click to insert your answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Complete the following **Newton's method** function.\n",
    " - You need to implement the update rule for Newton's method where you already calculated the Hessian matrix and the first gradient. The update rule is:\n",
    "    ### <center> $\\textbf{w}^k = \\textbf{w}^{k-1} - (\\nabla^2 g(\\textbf{w}^{k-1}))^{-1} \\nabla g(\\textbf{w}^{k-1})$ <center>\n",
    "  \n",
    " - You can use np.linalg.inv() function to calculate the inverse of Hessian. Be careful with the dimensions for matrix multiplications as the dimension of w should be $2 \\times 1$ this time.\n",
    " - You need to take steps until the stopping criteria is met. **Stopping criteria** is the Euclidian distance between consecutive w vectors to get below 1e-02.\n",
    "    - Write the gradient descent loop as it breaks when the Euclidian distance between $w_{k-1}$ and $w_{k}$ is smaller than 1e-02. You may not include $w_{k}$ in the weight history to calculate the number of iterations precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewtonsMethod_func2(g, w_0 = None):\n",
    "    '''\n",
    "        w_0: initial w\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    epsilon = 1e-6 # Add it to denominator of update rule to avoid division by zero \n",
    "    \n",
    "    w = w_0\n",
    "    w_history = [w] #List of w's in each iteration, used to plot\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    # Code the gradient functions here.\n",
    "    grad_1st = ...\n",
    "    H = ...\n",
    "    inv_H = ...\n",
    "    \n",
    "    #Gradient Descent Steps until the stopping criteria is met\n",
    "    \n",
    "    ...\n",
    "        \n",
    "        \n",
    "    ### YOUR CODE ENDS HERE    \n",
    "        \n",
    "    return w, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run and test the Newton's method function with different w_0 values and plot the weight path at the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'ellipsis' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6b09ee833e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mw_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplotFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0e2929262072>\u001b[0m in \u001b[0;36mplotFunction\u001b[0;34m(g, w_history, view, w_min, w_max)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdraw_weight_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0e2929262072>\u001b[0m in \u001b[0;36mdraw_weight_path\u001b[0;34m(ax, w_hist, g)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# plot each weight set as a point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolorspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medgecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# plot connector between points for visualization purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3ddae978aff6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We define the function 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now, we can call the plotFunction to plot surface plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplotFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'ellipsis' and 'int'"
     ]
    }
   ],
   "source": [
    "w, w_history = NewtonsMethod_func2(g, w_0=np.array([[...],[...]]))\n",
    "\n",
    "# Squeeze w vectors for plotting\n",
    "for i in range(len(w_history)):\n",
    "    w = w_history[i]\n",
    "    w_history[i] = np.squeeze(w)\n",
    "\n",
    "plotFunction(g, w_history, view=(40,-40),w_min=-10, w_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Complete the gradient function below for optimizing the function 2. You already calculated the first derivative, you can use it directly. Stopping criteria is the same with the Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent_func2(g, lr = 1e-1, w_0 = None):\n",
    "    '''\n",
    "        lr: learning rate\n",
    "        w_0: initial w\n",
    "    '''\n",
    "    #Initialize variables\n",
    "    w = w_0\n",
    "    w_history = [w] #List of w's in each iteration, used to plot\n",
    "\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    # Code the gradient(w) function here.\n",
    "    gradient = lambda w: ...\n",
    "    \n",
    "    #Gradient Descent Steps until the stopping criteria is met\n",
    "    \n",
    "    ...\n",
    "        \n",
    "        \n",
    "    ### YOUR CODE ENDS HERE    \n",
    "        \n",
    "    return w, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run and test the gradient descent function with different w_0 values and plot the weight path at the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, w_history = gradientDescent_func2(g, lr= 0.1, w_0=np.array([[...],[...]]))\n",
    "\n",
    "# Squeeze w vectors for plotting\n",
    "for i in range(len(w_history)):\n",
    "    w = w_history[i]\n",
    "    w_history[i] = np.squeeze(w)\n",
    "\n",
    "plotFunction(g, w_history, view=(40,-40),w_min=-10, w_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Fill in the tables below using different w_0 parameters. Step size for gradient descent is fixed as 0.1.\n",
    "\n",
    "| Newton's method   | Number of iterations | Found minimum |\n",
    "|-------------------|----------------------|---------------|\n",
    "| w0 = (9, 9)       |                      |               |\n",
    "| w0 = (7, 7)       |                      |               |\n",
    "| w0 = (5, 5)       |                      |               |\n",
    "| w0 = (3, 3)       |                      |               |\n",
    "| w0 = (1, 1)       |                      |               |\n",
    "\n",
    "| Gradient Descent  | Number of iterations | Found minimum |\n",
    "|-------------------|----------------------|---------------|\n",
    "| w0 = (9, 9)       |                      |               |\n",
    "| w0 = (7, 7)       |                      |               |\n",
    "| w0 = (5, 5)       |                      |               |\n",
    "| w0 = (3, 3)       |                      |               |\n",
    "| w0 = (1, 1)       |                      |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Compare and interpret the results. What did you observe? Which one of the approaches was better? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Double click here to insert your answer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Gradient Descent for Logistic Regression (15 Points)\n",
    "\n",
    "### Background\n",
    "\n",
    " - You can refer to Machine Learning Blinks 6 for related lecture video: https://www.youtube.com/watch?v=vZxE5XUdFyM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ&index=25\n",
    "\n",
    "In logistic regression, the aim is to minimize the \"Least Squares cost\" function, which is:\n",
    "\n",
    "$g(b, w)=\\sum_{p=1}^{P}\\left(\\sigma\\left(b+\\mathbf{x}_{p}^{T} \\mathbf{w}\\right)-y_{p}\\right)^{2}$\n",
    "\n",
    "where $\\mathbf{x}_{p}=\\left[\\begin{array}{llll}x_{1, p} & x_{2, p} & \\dots & x_{N, p}\\end{array}\\right]^{T}$ and $\\mathbf{w}_{p}=\\left[\\begin{array}{llll}w_{1} & w_{2} & \\dots & w_{N}\\end{array}\\right]^{T}$.\n",
    "\n",
    "The gradient descent step with respect to above Least Squares cost function is then:\n",
    "\n",
    "$\\Delta g(\\tilde{w})=2 \\sum_{p=1}^{P}\\left(\\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)-y_{p}\\right) \\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)\\left(1-\\sigma\\left(\\tilde{x}_{p}^{T} \\tilde{w}\\right)\\right) \\tilde{x}_{p}$\n",
    "\n",
    "This step can be written more compactly by denoting\n",
    "\n",
    "$\\begin{aligned} \\sigma_{p}^{k-1} &=\\sigma\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}^{k-1}\\right) \\\\ r_{p}^{k-1} &=2\\left(\\sigma_{p}^{k-1}-y_{p}\\right) \\sigma_{p}^{k-1}\\left(1-\\sigma_{p}^{k-1}\\right) \\end{aligned}$\n",
    "\n",
    "for all $p=1, \\ldots, P$ and $\\mathbf{r}^{k-1}=\\left[\\begin{array}{llll}r_{1}^{k-1} & r_{2}^{k-1} & \\ldots & r_{P}^{k-1}\\end{array}\\right]^{T}$, and stacking the $\\tilde{\\mathbf{x}}_{p}$ column-wise into the matrix $\\tilde{\\mathbf{X}}$. Then the gradient can be written as:\n",
    "\n",
    "$\\Delta g\\left(\\tilde{\\mathbf{w}}^{k-1}\\right)=\\tilde{\\mathbf{X}} \\mathbf{r}^{k-1}$\n",
    "\n",
    "For programming languages like Python and MATLAB/OCTAVE that have especially efficient implementations of matrix/vector operations this can be much more efficient than explicitly summing over the P points. In other words, these steps are shown for programming simplicity on Python/Matlab like languages.\n",
    "\n",
    "### Complete gradient descent function\n",
    "\n",
    "In this exercise you will reproduce the gradient descent paths shown in the Figure below:\n",
    "\n",
    "<img src=\"https://web.itu.edu.tr/kamard/Fig3_1_note.png\" style=\"width: 600px;\" />\n",
    "\n",
    "The surface in this figure was generated using the code below with the dataset $bacteria\\_data.csv$, and you must complete a short gradient descent function to produce the descent paths called\n",
    "\n",
    "$[i n, o u t]=g r a d_{-} d e s c e n t\\left(\\tilde{\\mathbf{X}}, \\mathbf{y}, \\tilde{\\mathbf{w}}^{0}\\right)$\n",
    "\n",
    "where \"in\" and \"out\" contain the gradient steps $\\tilde{\\mathbf{w}}^{k}=\\tilde{\\mathbf{w}}^{k-1}-\\alpha_{k} \\nabla g\\left(\\tilde{\\mathbf{w}}^{k-1}\\right)$ taken corresponding objective value $g\\left(\\tilde{\\mathbf{w}}^{k}\\right)$ respectively, $\\tilde{\\mathbf{X}}$ is the input data matrix, $y$ the output values and $\\tilde{\\mathbf{w}}^{0}$ the initial point.\n",
    "\n",
    "Almost all of this function has already been constructed for you. For example, the step length is fixed at $\\alpha_{k}=10^{-2}$ for all iterations, etc., and you must only enter the gradient of the associated cost function. Pressing \"run\" in the editor will run gradient descent and will reproduce __Figure 1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_data(csvname):\n",
    "    data = np.asarray(pd.read_csv(csvname,header = None))\n",
    "    x = data[:,0]\n",
    "    x.shape = (np.size(x),1)\n",
    "    temp = np.ones((np.size(x),1))\n",
    "    X = np.concatenate((temp,x),1)\n",
    "    y = data[:,1]\n",
    "    y = y/y.max()\n",
    "    y.shape = (np.size(y),1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality required for a proper gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE - COMPLETE THE GRADIENT DESCENT CODE ###\n",
    "# run gradient descent\n",
    "def gradient_descent(X,y,w0):\n",
    "    w_path = []                 # container for weights learned at each iteration\n",
    "    cost_path = []              # container for associated objective values at each iteration\n",
    "    w_path.append(w0)\n",
    "    cost = compute_cost(w0)\n",
    "    cost_path.append(cost)\n",
    "    w = w0\n",
    "\n",
    "    # start gradient descent loop\n",
    "    max_its = 5000\n",
    "    alpha = 10**(-2)\n",
    "    for k in range(max_its):\n",
    "        # compute gradient  \n",
    "        grad = ...\n",
    "        \n",
    "        # take gradient step\n",
    "        w = w - alpha*grad\n",
    "\n",
    "        # update path containers\n",
    "        w_path.append(w)\n",
    "        cost = compute_cost(w)\n",
    "        cost_path.append(cost)\n",
    "\n",
    "    # reshape containers for use in plotting in 3d\n",
    "    w_path = np.asarray(w_path)\n",
    "    w_path.shape = (np.shape(w_path)[0],np.shape(w_path)[1])\n",
    "    return w_path,cost_path\n",
    "\n",
    "# calculate the cost value for a given input weight w\n",
    "def compute_cost(w):\n",
    "    temp = 1/(1 + my_exp(-np.dot(X,w))) - y\n",
    "    temp = np.dot(temp.T,temp)\n",
    "    return temp[0][0]\n",
    "\n",
    "# avoid overflow when using exp - just cutoff after arguments get too large/small\n",
    "def my_exp(u):\n",
    "    s = np.argwhere(u > 100)\n",
    "    t = np.argwhere(u < -100)\n",
    "    u[s] = 0\n",
    "    u[t] = 0\n",
    "    u = np.exp(u)\n",
    "    u[t] = 1\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions necessary to producing the data, fit, cost surface, and descent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by plot_logistic_surface to make objective surface of logistic regression cost function\n",
    "def add_layer(a,b,c):\n",
    "    a.shape = (2,1)\n",
    "    b.shape = (1,1)\n",
    "    z = my_exp(-np.dot(c,a))\n",
    "    z = 1/(1 + z) - b\n",
    "    z = z**2\n",
    "    return z\n",
    "\n",
    "# plot fit to data and corresponding gradient descent path onto the logistic regression objective surface\n",
    "def show_fit(w_path,ax,col):\n",
    "    # plot solution of gradient descent fit to original data\n",
    "    s = np.linspace(0,25,100)\n",
    "    t = 1/(1 + my_exp(-(w_path[-1][0] + w_path[-1][1]*s)))\n",
    "    ax.plot(s,t,color = col)\n",
    "\n",
    "# plot gradient descent paths on cost surface\n",
    "def show_paths(w_path,cost_path,ax,col):           \n",
    "    # plot grad descent path onto surface\n",
    "    ax.plot(w_path[:,0],w_path[:,1],cost_path,color = col,linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n",
    "    \n",
    "# plot logistic regression surface\n",
    "def plot_surface(ax):\n",
    "    # plot logistic regression surface\n",
    "    r = np.linspace(-3,3,100)\n",
    "    s,t = np.meshgrid(r, r)\n",
    "    s = np.reshape(s,(np.size(s),1))\n",
    "    t = np.reshape(t,(np.size(t),1))\n",
    "    h = np.concatenate((s,t),1)\n",
    "\n",
    "    # build 3d surface\n",
    "    surf = np.zeros((np.size(s),1))\n",
    "    max_its = np.size(y)\n",
    "    for i in range(0,max_its):\n",
    "        surf = surf + add_layer(X[i,:],y[i],h)\n",
    "\n",
    "    # reshape \n",
    "    s = np.reshape(s,(100,100))\n",
    "    t = np.reshape(t,(100,100))\n",
    "    surf = np.reshape(surf,(100,100))\n",
    "\n",
    "    # plot 3d surface\n",
    "    ax.plot_surface(s,t,surf,cmap = 'jet')\n",
    "    ax.azim = 175\n",
    "    ax.elev = 20\n",
    "    \n",
    "# plot points\n",
    "def plot_points(X,y,ax):\n",
    "    ax.plot(X[:,1],y,'ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is defined we can run all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X,y = load_data('bacteria_data.csv') # load in data\n",
    "\n",
    "# initialize figure, plot data, and dress up panels with axes labels etc.,\n",
    "fig = plt.figure(facecolor = 'white',figsize = (8,3))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlim(min(X[:,1])-0.5, max(X[:,1])+0.5)\n",
    "ax1.set_ylim(min(y)-0.1,max(y)+0.1)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.xaxis.set_rotate_label(False)\n",
    "ax2.yaxis.set_rotate_label(False)\n",
    "ax2.zaxis.set_rotate_label(False)\n",
    "ax2.get_xaxis().set_ticks([-3,-1,1,3])\n",
    "ax2.get_yaxis().set_ticks([-3,-1,1,3])\n",
    "# ax2.axis('off')\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0)\n",
    "\n",
    "# plot points\n",
    "plot_points(X,y,ax1)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'m')\n",
    "show_paths(w_path,cost_path,ax2,'m')\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,-2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'c')\n",
    "show_paths(w_path,cost_path,ax2,'c')\n",
    "plot_surface(ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gradient Descent for $\\ell_{2}$ Regularized Logistic Regression (10 Points)\n",
    "\n",
    "### Background\n",
    "\n",
    "- You can refer to Machine Learning Blinks 6 for related lecture video: https://www.youtube.com/watch?v=vZxE5XUdFyM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ&index=25\n",
    "\n",
    "The Least Squares cost function of $\\ell_{2}$ regularized Logistic regression is:\n",
    "\n",
    "$g(b, w)=\\sum_{p=1}^{P}\\left(\\sigma\\left(b+x_{p}^{T} w\\right)-y_{p}\\right)^{2}+\\lambda\\|w\\|_{2}^{2}$\n",
    "\n",
    "The gradient of the cost function can be written as: \n",
    "\n",
    "$\\nabla g(\\tilde{\\mathbf{w}})=2 \\sum_{p=1}^{P}\\left(\\sigma\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}\\right)-y_{p}\\right) \\sigma\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}\\right)\\left(1-\\sigma\\left(\\tilde{\\mathbf{x}}_{p}^{T} \\tilde{\\mathbf{w}}\\right)\\right) \\tilde{\\mathbf{x}}_{\\mathbf{p}}+2 \\lambda\\left[\\begin{array}{l}0 \\\\ \\mathbf{w}\\end{array}\\right]$\n",
    "\n",
    "### Complete the gradient descent function\n",
    "\n",
    "<img src=\"https://web.itu.edu.tr/kamard/Fig3_2_note.png\" style=\"width: 600px;\" />\n",
    "\n",
    "In this exercise you will reproduce __Figure 2__ by coding up gradient descent to minimize the regularized logistic regression Least Squares cost function. The surface in the figure was generated using the code below with the dataset $bacteria\\_data.csv$, and you must complete a short gradient descent function to produce the descent paths called\n",
    "\n",
    "$[i n, o u t]=g r a d_{-} d e s c e n t\\left(\\tilde{\\mathbf{X}}, \\mathbf{y}, \\tilde{\\mathbf{w}}^{0}\\right)$\n",
    "\n",
    "where \"in\" and \"out\" contain the gradient steps $\\tilde{\\mathbf{w}}^{k}=\\tilde{\\mathbf{w}}^{k-1}-\\alpha_{k} \\nabla g\\left(\\tilde{\\mathbf{w}}^{k-1}\\right)$ taken corresponding objective value $g\\left(\\tilde{\\mathbf{w}}^{k}\\right)$ respectively, $\\tilde{\\mathbf{X}}$ is the input data matrix, $y$ the output values and $\\tilde{\\mathbf{w}}^{0}$ the initial point.\n",
    "\n",
    "Almost all of this function has already been constructed for you. For example, the step length is fixed at $\\alpha_{k}=10^{-2}$ for all iterations, etc., and you must only enter the gradient of the associated cost function. Pressing \"run\" in the editor will run gradient descent and will reproduce __Figure 2__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_data(csvname):\n",
    "    data = np.asarray(pd.read_csv(csvname,header = None))\n",
    "    x = data[:,0]\n",
    "    x.shape = (np.size(x),1)\n",
    "    temp = np.ones((np.size(x),1))\n",
    "    X = np.concatenate((temp,x),1)\n",
    "    y = data[:,1]\n",
    "    y = y/y.max()\n",
    "    y.shape = (np.size(y),1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality required for a proper gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE - COMPLETE THE GRADIENT DESCENT FUNCTION ###\n",
    "# run gradient descent\n",
    "def gradient_descent(X,y,w0,lam):\n",
    "    w_path = []                 # container for weights learned at each iteration\n",
    "    cost_path = []              # container for associated objective values at each iteration\n",
    "    w_path.append(w0)\n",
    "    cost = compute_cost(w0)\n",
    "    cost_path.append(cost)\n",
    "    w = w0\n",
    "\n",
    "    # start gradient descent loop\n",
    "    max_its = 5000\n",
    "    alpha = 10**(-2)\n",
    "    for k in range(max_its):\n",
    "        # compute gradient\n",
    "        grad = ...\n",
    "\n",
    "        # take gradient step\n",
    "        w = w - alpha*grad\n",
    "\n",
    "        # update path containers\n",
    "        w_path.append(w)\n",
    "        cost = compute_cost(w)\n",
    "        cost_path.append(cost)\n",
    "\n",
    "    # reshape containers for use in plotting in 3d\n",
    "    w_path = np.asarray(w_path)\n",
    "    w_path.shape = (np.shape(w_path)[0],np.shape(w_path)[1])\n",
    "    return w_path,cost_path\n",
    "\n",
    "# calculate the cost value for a given input weight w\n",
    "def compute_cost(w):\n",
    "    temp = 1/(1 + my_exp(-np.dot(X,w))) - y\n",
    "    temp = np.dot(temp.T,temp)\n",
    "    return temp[0][0]\n",
    "\n",
    "# avoid overflow when using exp - just cutoff after arguments get too large/small\n",
    "def my_exp(u):\n",
    "    s = np.argwhere(u > 100)\n",
    "    t = np.argwhere(u < -100)\n",
    "    u[s] = 0\n",
    "    u[t] = 0\n",
    "    u = np.exp(u)\n",
    "    u[t] = 1\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions necessary to producing the data, fit, cost surface, and descent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by plot_logistic_surface to make objective surface of logistic regression cost function\n",
    "def add_layer(a,b,c):\n",
    "    a.shape = (2,1)\n",
    "    b.shape = (1,1)\n",
    "    z = my_exp(-np.dot(c,a))\n",
    "    z = 1/(1 + z) - b\n",
    "    z = z**2\n",
    "    return z\n",
    "\n",
    "# plot fit to data and corresponding gradient descent path onto the logistic regression objective surface\n",
    "def show_fit(w_path,ax,col):\n",
    "    # plot solution of gradient descent fit to original data\n",
    "    s = np.linspace(0,25,100)\n",
    "    t = 1/(1 + my_exp(-(w_path[-1,0] + w_path[-1,1]*s)))\n",
    "    ax.plot(s,t,color = col)\n",
    "\n",
    "# plot gradient descent paths on cost surface\n",
    "def show_paths(w_path,cost_path,ax,col):           \n",
    "    # plot grad descent path onto surface\n",
    "    ax.plot(w_path[:,0],w_path[:,1],cost_path,color = col,linewidth = 5)   # add a little to output path so its visible on top of the surface plot\n",
    "    \n",
    "# plot logistic regression surface\n",
    "def plot_surface(ax,lam):\n",
    "    # plot logistic regression surface\n",
    "    r = np.linspace(-3,3,100)\n",
    "    s,t = np.meshgrid(r, r)\n",
    "    s = np.reshape(s,(np.size(s),1))\n",
    "    t = np.reshape(t,(np.size(t),1))\n",
    "    h = np.concatenate((s,t),1)\n",
    "\n",
    "    # build 3d surface\n",
    "    surf = np.zeros((np.size(s),1))\n",
    "    max_its = np.size(y)\n",
    "    for i in range(0,max_its):\n",
    "        surf = surf + add_layer(X[i,:],y[i],h)\n",
    "    surf = surf + lam*t**2\n",
    "\n",
    "    s = np.reshape(s,(100,100))\n",
    "    t = np.reshape(t,(100,100))\n",
    "    surf = np.reshape(surf,(100,100))\n",
    "    \n",
    "    # plot 3d surface\n",
    "    ax.plot_surface(s,t,surf,cmap = 'jet')\n",
    "    ax.azim = 175\n",
    "    ax.elev = 20\n",
    "    \n",
    "# plot points\n",
    "def plot_points(X,y,ax):\n",
    "    ax.plot(X[:,1],y,'ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is defined we can run all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X,y = load_data('bacteria_data.csv') # load in data\n",
    "\n",
    "# initialize figure, plot data, and dress up panels with axes labels etc.,\n",
    "fig = plt.figure(facecolor = 'white',figsize = (8,3))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlim(min(X[:,1])-0.5, max(X[:,1])+0.5)\n",
    "ax1.set_ylim(min(y)-0.1,max(y)+0.1)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.xaxis.set_rotate_label(False)\n",
    "ax2.yaxis.set_rotate_label(False)\n",
    "ax2.zaxis.set_rotate_label(False)\n",
    "ax2.get_xaxis().set_ticks([-3,-1,1,3])\n",
    "ax2.get_yaxis().set_ticks([-3,-1,1,3])\n",
    "# ax2.axis('off')\n",
    "\n",
    "# define regularizer parameter\n",
    "lam = 10**-1\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0,lam)\n",
    "\n",
    "# plot points\n",
    "plot_points(X,y,ax1)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'m')\n",
    "show_paths(w_path,cost_path,ax2,'m')\n",
    "\n",
    "### run gradient descent with first initial point\n",
    "w0 = np.array([0,-2])\n",
    "w0.shape = (2,1)\n",
    "w_path, cost_path = gradient_descent(X,y,w0,lam)\n",
    "\n",
    "# plot fit to data and path on objective surface\n",
    "show_fit(w_path,ax1,'c')\n",
    "show_paths(w_path,cost_path,ax2,'c')\n",
    "plot_surface(ax2,lam)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Watt, J., Borhani, R., and Katsaggelos, A. (2016). Machine Learning Refined: Foundations, Algorithms, and Applications. Cambridge: Cambridge University Press. doi:10.1017/CBO9781316402276\n",
    "\n",
    "[2] Jianqiang Lin, Sang-Mok Lee, Ho-Joon Lee, and Yoon-Mo Koo. Modeling of typical microbial cell growth in batch culture. Biotechnology and Bioprocess Engineering, 5(5) 382385, 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
